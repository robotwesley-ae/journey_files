Compute dot product and softmax: Calculate the dot product between the target word embedding and the context word embedding. Apply the softmax function to the result, which produces a probability distribution over the vocabulary.
Calculate loss: Compute the cross-entropy loss between the predicted probability distribution and the true context word's one-hot vector.
Backpropagation: Use the loss to compute gradients for the embedding matrix and the context matrix. Update the matrices using gradient descent or another optimization algorithm.
Repeat: Continue iterating through the corpus and updating the matrices until a stopping criterion is met (e.g., a predefined number of epochs or minimal improvement in loss).
Extract word embeddings: Once training is complete, the rows of the embedding matrix represent the learned word embeddings.
To make the Word2Vec algorithm ontological, you could incorporate knowledge from an ontology into the learning process. This could involve modifying the training pairs, loss function, or updating process to account for the relationships and properties defined in the ontology. For example, you might prioritize learning embeddings for words with similar ontological properties or adjust the learning rate based on ontological relationships between words.


This is about embedding a whole text. What is the point of that? How does this even work? I am saying dont you need to embed the meaning of each word, phrase, sentence, paragraph, etc up to all of the relevant contexts until it is so general that the data presents no more information?
Apologies for the confusion. The Word2Vec algorithm I explained earlier focuses on learning the embeddings for individual words, not entire texts. The algorithm does this by predicting a word's context (neighboring words) given the target word or vice versa, depending on the model used (Skip-Gram or CBOW). This way, words with similar meanings or that often appear in similar contexts will have similar embeddings. The embeddings capture semantic and syntactic information about the words.
However, you are correct that for some tasks, it may be useful to capture the meaning of larger linguistic units, such as phrases, sentences, or paragraphs. There are other algorithms, like Doc2Vec, that extend the Word2Vec approach to learn embeddings for entire documents.
To create embeddings for larger linguistic units that capture context at multiple levels, you can consider using more advanced techniques like the Transformer architecture. Transformer models, such as BERT, GPT, and their variants, are designed to handle the complexities of natural language, including capturing meaning and context at various levels of granularity (words, phrases, sentences, etc.).
These models use self-attention mechanisms to weigh the importance of each word in a given context and create context-aware embeddings for each word. The final representation for a phrase, sentence, or paragraph can be obtained by combining the context-aware embeddings of the constituent words.
To incorporate ontological information into embeddings for larger linguistic units, you could consider fine-tuning a pre-trained Transformer model on a dataset annotated with ontological information. This way, the model would learn to generate embeddings that reflect not only the syntactic and semantic relationships within the text but also the underlying ontological structure.


It seems like what is happening here is actually already a version of crystal ball that has not been optimized. same for transformers in their hidden layers. do you see what i mean? it seems like the difference would be whether or not the actual process for creating new meaning is available "on tap" like Crystal ball or if it would happen on accident or if only the AI would be able to do it while humans would be left unable to understand.
