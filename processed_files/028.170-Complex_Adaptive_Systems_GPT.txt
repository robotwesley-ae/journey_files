I apologize if my previous response was unclear or not aligned with your goal. It seems like you want to create an embedding that captures both the one-hot encoding and the ontological relationships in a more compact representation. Let me propose an alternative approach:
Create an ontological graph where nodes represent entities (words) and edges represent ontological relationships between them. Each node contains the full bit representation of the entity.
For each entity, obtain a one-hot encoding that represents its unique position in the vocabulary.
Use an embedding method, such as Word2Vec, to learn dense embeddings that capture the co-occurrence and semantic relationships between words in the text.
Combine the one-hot encoding and the learned dense embeddings by concatenating them. This would result in a new embedding that captures both the unique position of the word in the vocabulary and the semantic relationships with other words.
Use a dimensionality reduction technique, such as PCA or t-SNE, to reduce the size of the concatenated embeddings while preserving as much information as possible about the ontological relationships and one-hot encoding.
This approach would create an embedding that combines the one-hot encoding and the semantic relationships in a more compact representation. However, this method may not provide a direct mapping from the embedding back to the full bit representation of the entities. You may need to experiment with different techniques and weightings to find the most effective way to encode the desired information in the embeddings.


thats not what i meant sorry.  nevermind. i dont understand it.
I apologize if my previous explanations didn't address your concern adequately. If you would like, we can revisit the topic or move on to another aspect of the system you're designing. Please feel free to ask any further questions or provide more context on what you'd like to explore. I'm here to help.


Here are my questions about what you said:

Input: Word2Vec takes a large corpus of text as input, preprocesses it by tokenizing words, and possibly removing stop words and applying stemming or lemmatization.
Ok so in this step it is ontologically classified. 
Model architecture: Word2Vec uses a simple neural network with an input layer, a hidden layer, and an output layer. The input and output layers have the same size, equal to the vocabulary size V, and the hidden layer has a lower dimension N, typically in the range of 100-300.

One-hot encoding: Each word in the vocabulary is represented by a one-hot encoded vector of size V. The input to the model is the one-hot encoded vector of the target word, and the output is a probability distribution over the vocabulary, representing the likelihood of each word being in the context of the target word.
Is V an arbitrary limit imposed for the sake of limiting the processing requirements?
And this process is automatically completed no matter what the input is, but doesnt make sense if the input text is non-sensical? 
Training: The training process involves moving a window of fixed size over the corpus. For each target word, we create training pairs with its surrounding context words. The objective is to maximize the probability of observing context words given the target word. This is achieved by minimizing the following loss function:

L = -âˆ‘(y_actual * log(y_predicted))

Where y_actual is the one-hot encoded vector of the actual context word, and y_predicted is the probability distribution generated by the model for the target word.

Optimization: The weights of the neural network are updated using an optimization algorithm, such as stochastic gradient descent or Adam, to minimize the loss function.

Training is "complete" when the loss-function is minimized to the target level?

Word embeddings: After training, the weights of the input layer (input-hidden layer) are used as word embeddings. Each row of the weight matrix corresponds to the word embedding of a specific word in the vocabulary.

So to be clear, the training data is then used as an embedding in the hidden layer, such that:
