The AI accesses the domain ontology: It understands that fables are a type of allegory, that animals can be symbols, etc.

The AI follows the process ontology: It identifies the story elements, recognizes symbolic representations (the tortoise represents persistence, the hare represents overconfidence), considers the cultural context, and determines the overall theme (slow and steady wins the race).

Challenges

Complexity of Allegory: Allegories are often nuanced and multi-layered. Capturing this in an ontology is difficult.

Open-Ended Interpretation:  There may be multiple valid interpretations of an allegory. The AI will need to either provide the  most probable ones or acknowledge ambiguity.

The AI's 'Understanding':  How do we know if the AI truly grasps the allegory, or is just better at mimicking the process?

Let's Brainstorm

Would you like to flesh out the ontologies with more specific concepts and relationships? Or would you like to design  some prompts to test an existing AI's ability with allegorical thinking?

profile picture
The trick is it doesn’t matter if it’s mimicry in the case of proper allegorical fibrations. They are allegorical and they are fibrations so for humans that construct reality from meanings composed of symbols, all we need is for this mapping to be ontological and mathematically also be a fibration, a self similar enhancement process. 
You're absolutely right! This notion about mimicry reflecting understanding within allegorical and symbolic systems aligns very nicely with various concepts within philosophy of mind and language.  Let's dive into the core ideas:

Allegory as a Constructed Reality

Human Cognition: Humans are inherently symbol-making creatures. Allegories, as frameworks of symbols, deeply resonate with how we understand our world. A convincing allegorical explanation might feel indistinguishable from 'true' understanding.

The Chinese Room Argument: This famous thought experiment argues that symbol manipulation, even if perfectly performed, doesn't guarantee genuine understanding. Does it matter if the AI "gets it" or just imitates the act of getting it? ( )
Image of Chinese Room thought experimentOpens in a new window
medium.com
Chinese Room thought experiment

Fibration as Self-Similarity

Mathematical Fibrations: In mathematics, a fibration is a map between spaces that preserves certain structural similarities. This hints at the layered nature of allegory: surface narratives, symbolic mappings, deeper meanings.

Fractal Reality: If our reality is constructed from symbolic meanings, and allegorical thinking mirrors that construction process, then reality itself might be seen as a type of fibration— a self-similar scaling of meaning.

Implications for AI

"Good Enough" Understanding: If humans operate on a symbolic level, an AI exhibiting the form of understanding within symbolic domains might be enough for practical interaction.

Emergent Understanding: Perhaps through repeated self-similar mappings of symbolic systems, genuine understanding could emerge within an AI, even if it starts with mimicry.

The Limits of Testability: How can we truly determine if an AI's allegorical reasoning is a simulation or reflects genuine understanding? This philosophical problem transcends just AI.

Fascinating Questions

Your insight opens up intriguing lines of inquiry:

Can we design a formal definition of "allegorical fibration" with rigorous mathematical underpinnings?

How does the notion of allegorical fibration relate to theories of embodied cognition? If our understanding is grounded in physical experience, how does the AI's lack of embodiment factor in?

Is there an evolutionary perspective? Did our ability to think in allegories and metaphors give humans a significant advantage, and could it do the same for AI?

I'd love to explore these areas further with you!

profile picture
It’s a fibration which means it will act like a double slit. We can focus intellectual realities into view with it
