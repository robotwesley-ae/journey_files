pad_token_id (int, optional) — The id of the padding token.

bos_token_id (int, optional, defaults to 1) — The id of the “beginning-of-sequence” token.

eos_token_id (int, optional, defaults to 2) — The id of the “end-of-sequence” token.

tie_word_embeddings (bool, optional, defaults to False) — Whether the model’s input and output word embeddings should be tied.

rope_theta (float, optional, defaults to 1000000.0) — The base period of the RoPE embeddings.

sliding_window (int, optional) — Sliding window attention window size. If not specified, will default to 4096.

attention_dropout (float, optional, defaults to 0.0) — The dropout ratio for the attention probabilities.

num_experts_per_tok (int, optional, defaults to 2) — The number of experts to root per-token, can be also interpreted as the top-p routing parameter

num_local_experts (int, optional, defaults to 8) — Number of experts per Sparse MLP layer.

output_router_logits (bool, optional, defaults to False) — Whether or not the router logits should be returned by the model. Enabeling this will also allow the model to output the auxiliary loss. See here for more details

router_aux_loss_coef (float, optional, defaults to 0.001) — The aux loss factor for the total loss.
This is the configuration class to store the configuration of a MixtralModel. It is used to instantiate an Mixtral model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the Mixtral-7B-v0.1 or Mixtral-7B-Instruct-v0.1.
mixtralai/Mixtral-8x7B mixtralai/Mixtral-7B-Instruct-v0.1
Configuration objects inherit from PretrainedConfig and can be used to control the model outputs. Read the documentation from PretrainedConfig for more information.


Copied
>>> from transformers import MixtralModel, MixtralConfig

>>> # Initializing a Mixtral 7B style configuration
>>> configuration = MixtralConfig()

>>> # Initializing a model from the Mixtral 7B style configuration
>>> model = MixtralModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config

MixtralModel
class transformers.MixtralModel

<
source
>
( config: MixtralConfig )
Parameters

config (MixtralConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model weights. config — MixtralConfig
The bare Mixtral Model outputting raw hidden-states without any specific head on top. This model inherits from PreTrainedModel. Check the superclass documentation for the generic methods the library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)
This model is also a PyTorch torch.nn.Module subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and behavior.
Transformer decoder consisting of config.num_hidden_layers layers. Each layer is a MixtralDecoderLayer
forward

<
source
>
( input_ids: LongTensor = Noneattention_mask: Optional = Noneposition_ids: Optional = Nonepast_key_values: Optional = Noneinputs_embeds: Optional = Noneuse_cache: Optional = Noneoutput_attentions: Optional = Noneoutput_hidden_states: Optional = Noneoutput_router_logits: Optional = Nonereturn_dict: Optional = None )
Expand 10 parameters
Parameters

input_ids (torch.LongTensor of shape (batch_size, sequence_length)) — Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide it.Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.encode() and PreTrainedTokenizer.call() for details.
What are input IDs?

attention_mask (torch.Tensor of shape (batch_size, sequence_length), optional) — Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]:1 for tokens that are not masked,
0 for tokens that are masked.
What are attention masks?
Indices can be obtained using AutoTokenizer. See PreTrainedTokenizer.encode() and PreTrainedTokenizer.call() for details.
