Input: Word2Vec takes a large corpus of text as input, preprocesses it by tokenizing words, and possibly removing stop words and applying stemming or lemmatization.
Model architecture: Word2Vec uses a simple neural network with an input layer, a hidden layer, and an output layer. The input and output layers have the same size, equal to the vocabulary size V, and the hidden layer has a lower dimension N, typically in the range of 100-300.
One-hot encoding: Each word in the vocabulary is represented by a one-hot encoded vector of size V. The input to the model is the one-hot encoded vector of the target word, and the output is a probability distribution over the vocabulary, representing the likelihood of each word being in the context of the target word.
Training: The training process involves moving a window of fixed size over the corpus. For each target word, we create training pairs with its surrounding context words. The objective is to maximize the probability of observing context words given the target word. This is achieved by minimizing the following loss function:
L = -âˆ‘(y_actual * log(y_predicted))
Where y_actual is the one-hot encoded vector of the actual context word, and y_predicted is the probability distribution generated by the model for the target word.
Optimization: The weights of the neural network are updated using an optimization algorithm, such as stochastic gradient descent or Adam, to minimize the loss function.
Word embeddings: After training, the weights of the input layer (input-hidden layer) are used as word embeddings. Each row of the weight matrix corresponds to the word embedding of a specific word in the vocabulary.
To incorporate ontological information into Word2Vec, you can modify the algorithm in the following ways:
Instead of using raw text as input, use text that is annotated with ontological relationships and properties.
Modify the objective function to account for ontological information by adding terms or weights related to the ontological relationships between words.
Introduce an additional constraint or regularization term in the loss function to ensure that the generated embeddings respect the ontological relationships and properties.
During training, update the embeddings based on both the context word probabilities and the ontological information.
By incorporating ontological information into the Word2Vec algorithm, you can create embeddings that better represent the underlying semantic relationships between words and the knowledge domain.


Yeah so the embedding im talking about seems to be concatenating the one-hot encoding such that the one-hot encoding happens on a different graph where we know the full bit representation of an entity and is then abstracted into a walk-unique symbol. Maybe not. What do you think?
Concatenating one-hot encodings might not be the most efficient way to represent the underlying semantic relationships, as it may result in a very high-dimensional and sparse vector. However, I understand that you want to incorporate the full bit representation of an entity and its ontological information into the word embedding.
Instead, you could consider the following approach to achieve this:
Create an ontological graph where nodes represent entities (words) and edges represent ontological relationships between them. Each node contains the full bit representation of the entity.
Perform graph-based learning, such as Graph Convolutional Networks (GCN) or Graph Attention Networks (GAT), to learn embeddings that capture the ontological relationships between entities. This process would involve propagating information along the edges of the graph and updating node embeddings based on their neighbors.
After training the graph-based model, you can use the learned embeddings as your ontological word embeddings. These embeddings should now encode the full bit representation of the


How? Thats the opposite effect of what I described, isnt it?
